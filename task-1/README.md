# Task 1 

<<<<<<< HEAD
=======
## Cross Validation
>>>>>>> task-1
In this directory there is an executible called `cross-validate`. This program will run a K-Fold Cross Validation on 4 possible neural networks:
1. A PSO optimised neural net with non-linear inputs (shape=(6,8,1))
2. A PSO optimised neural net with linear inputs (shape=(2,6,5,1))
3. A SGD optimised neural net with non-linear inputs (shape=(6,8,1))
4. A SGD optimised neural net with linear inputs (shape=(2,6,5,1))

The benefit of this executible is that each validation is run concurrently (see the Go code in the `cross_validator` directory).

To see the possible inputs, run `./cross-validate --help`. 

There are 3 parameters:
`-folds [int]` the number of folds for cross validation
`-optimiser [pso/sgd]` which optimiser to use when training
`-inputType [linear/nonlinear]` what type of inputs to use 

Running just `./cross-validate` will resuls in all four possible neural nets being trained on 10-fold cross validation.

To generate graphs, run `python plot_pso_loss.py`. 

In `output.json` you will find the results from each experiment. As an example, below is the output of running `./cross-validate`:
```
[
  {
    "iterations": 2,
    "optimizer": "pso",
    "input_type": "linear",
    "average_training_loss": 0.02617778752113721,
    "average_testing_loss": 0.041942438704346025,
    "average_fitness": 0.9501752357040496,
    "best_iteration": 1
  },
  {
    "iterations": 2,
    "optimizer": "pso",
    "input_type": "nonlinear",
    "average_training_loss": 0.012051342064804696,
    "average_testing_loss": 0.02984169535237023,
    "average_fitness": 0.9612631280038471,
    "best_iteration": 1
  },
  {
    "iterations": 2,
    "optimizer": "sgd",
    "input_type": "linear",
    "average_training_loss": 0.19817572087049484,
    "average_testing_loss": 0.20068246871232986,
    "average_fitness": 0.7980641573667526,
    "best_iteration": 1
  },
  {
    "iterations": 2,
    "optimizer": "sgd",
    "input_type": "nonlinear",
    "average_training_loss": 0.0397548321634531,
    "average_testing_loss": 0.04634080268442631,
    "average_fitness": 0.9503662120550871,
    "best_iteration": 1
  }
]
```

## Individual Training
It is advised to run the `cross-validate` executible at least until the first training starts to get the necessary datasets. However, the `data/` directory should already have the results of splitting the initial data in two pieces of equal size.

To run an individual training session, just use the following command: `python nn_[optimizer].py data/1_iter [non/linear]`

### Package requirements:
Please refer to the requirements.txt file (generated by pipreqs).

### Interesting Notes 
The PSO and the neural network optimised with PSO were coded from scratch using only numpy and standard python libraries. This was done because manually changing the weights for a pytorch neural network was code heavy and difficult to read; and, because the pytorch neural network works with backprobagation in mind. For the PSO backpropagation is not a consideration, so we only had to code forward propagation. Closures for both the neural network and the cost functions were used to ensure we had the proper function signatures to pass into the PSO class. This way, the PSO's code maintained its readability, and was able to stay close to the notation used in the lectures (which made it easier to debug). This code can be found in `nn_pso.py` and `pso.py`. 

We also decided that cross-validation would yield interesting results; however, the turnover for the training sessions were long, so we decided to try concurrent solutions. Python does not natively support concurrency, so we decided to use Golang which was designed with concurrency in mind. Golang handles the creation of the dataset files that are fed into the training scripts, and runs the scripts for each fold of the cross-validation concurrently, meaning that for a process that would take 10 minutes (5 per run), it instead takes maybe 6 or 7. Furthermore, making it so that all variants of the neural networks can be trained and tested automatically made the process of collecting results much less labour intensive than it otherwise would have been, as we were able to run the executible, and turn our attention to other elements of the assignment while the results were compiled.
